from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained BERT model and tokenizer
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Input text
input_text = "I am Computer Engineer"

# Tokenize input text
inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

# Run BERT model
with torch.no_grad():
    outputs = model(**inputs)

# Get hidden states (embeddings)
hidden_states = outputs.last_hidden_state

# Extract the contextual embedding for the [CLS] token (the first token)
cls_embedding = hidden_states[:, 0, :]

# Print results
print("Input Text:", input_text)
print("Contextual Embedding Shape:", cls_embedding.shape)
print("Contextual Embedding:", cls_embedding)
